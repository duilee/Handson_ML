{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pytorch_language_model_RNN.ipynb","provenance":[],"authorship_tag":"ABX9TyM1BdtA//2XIo8m0m7/0oam"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"FI5Y5DgYauwl"},"source":["# Language Model RNN"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K_jLCulxcCxW","executionInfo":{"status":"ok","timestamp":1618136370363,"user_tz":-540,"elapsed":19718,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}},"outputId":"7a6f84e1-0c58-471e-db5d-0ab2dd0832a7"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"gKrU4KtKbQzf","executionInfo":{"status":"ok","timestamp":1618136388714,"user_tz":-540,"elapsed":1065,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}},"outputId":"bbedf42d-af51-40d0-ab12-0330e2351195"},"source":["import os\n","\n","os.getcwd()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"RqwjT-bTcd3v","executionInfo":{"status":"ok","timestamp":1618136436248,"user_tz":-540,"elapsed":1039,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}}},"source":["os.chdir(\"./drive/My Drive/Colab Notebooks/pytorch\")"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"8yQgD7G7cpxn","executionInfo":{"status":"ok","timestamp":1618136446388,"user_tz":-540,"elapsed":1336,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}},"outputId":"9fc5bc18-7f96-42dd-c233-e13e9e98c865"},"source":["os.getcwd()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/Colab Notebooks/pytorch'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"zK6rAm9Acw2-"},"source":["## Create Dictionary, Corpus class"]},{"cell_type":"code","metadata":{"id":"GmNP90xxaVX4","executionInfo":{"status":"ok","timestamp":1618136496877,"user_tz":-540,"elapsed":992,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}}},"source":["import torch\n","import torch.nn as nn \n","import numpy as np\n","from torch.nn.utils import clip_grad_norm_"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"l13XLZ1ac0wz","executionInfo":{"status":"ok","timestamp":1618136723957,"user_tz":-540,"elapsed":1141,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}}},"source":["class Dictionary(object):\n","    def __init__(self):\n","        self.word2idx = {}\n","        self.idx2word = {}\n","        self.idx = 0\n","    \n","    def add_word(self, word):\n","        if not word in self.word2idx:\n","            self.word2idx[word] = self.idx\n","            self.idx2word[self.idx] = word\n","            self.idx += 1\n","    def __len__(self):\n","        return len(self.word2idx)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"6WAo35Ytdvel","executionInfo":{"status":"ok","timestamp":1618137289361,"user_tz":-540,"elapsed":1067,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}}},"source":["class Corpus(object):\n","    def __init__(self):\n","        self.dictionary = Dictionary()\n","    \n","    def get_data(self, path, batch_size=20):\n","        # Add words to the dictionary\n","        with open(path, 'r') as f:\n","            tokens = 0\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                tokens += len(words)\n","                for word in words:\n","                    self.dictionary.add_word(word)\n","        \n","        # Tokenize the file content\n","        ids = torch.LongTensor(tokens)\n","        token = 0\n","        with open(path, 'r') as f:\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                for word in words:\n","                    ids[token] = self.dictionary.word2idx[word]\n","                    token += 1\n","        \n","        num_batches = ids.size(0) // batch_size\n","        ids = ids[:num_batches*batch_size]\n","        return ids.view(batch_size, -1)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"DcYusNUnbEDT","executionInfo":{"status":"ok","timestamp":1618137617836,"user_tz":-540,"elapsed":1016,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}}},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"nXZPj9rihJEf","executionInfo":{"status":"ok","timestamp":1618137929070,"user_tz":-540,"elapsed":3208,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}}},"source":["# Hyper-parameters\n","embed_size = 128\n","hidden_size = 1024\n","num_layers = 1\n","num_epochs = 5\n","num_samples = 1000 # num of words to be samples\n","batch_size = 20\n","seq_length = 30\n","learning_rate = 0.002"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"D6ZbZVYbhlBX","executionInfo":{"status":"ok","timestamp":1618137941569,"user_tz":-540,"elapsed":5240,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}}},"source":["# Load \"Penn Treebank\" dataset\n","corpus = Corpus()\n","ids = corpus.get_data('train.txt', batch_size)\n","vocab_size = len(corpus.dictionary)\n","num_batches = ids.size(1) // seq_length"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Slir9XzuiXF4","executionInfo":{"status":"ok","timestamp":1618138843306,"user_tz":-540,"elapsed":966,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}},"outputId":"4b35df5c-2d7e-414e-d33e-b7a42e616c44"},"source":["ids[:, 0:2].detach()"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[   0,    1],\n","        [  93,  718],\n","        [  27,  930],\n","        [  48, 1523],\n","        [ 160, 1112],\n","        [4434,   32],\n","        [  24,  801],\n","        [2246,   32],\n","        [2006,  256],\n","        [ 365,  477],\n","        [ 467, 3150],\n","        [5092,   24],\n","        [ 220,   79],\n","        [ 480,   24],\n","        [  48,   26],\n","        [  27,   27],\n","        [ 853,   39],\n","        [ 997,   42],\n","        [ 392, 5518],\n","        [4210,  467]])"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"Kf6BTss1j89e","executionInfo":{"status":"ok","timestamp":1618139126213,"user_tz":-540,"elapsed":1120,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}}},"source":["## RNN based language model\n","class RNNLM(nn.Module):\n","    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n","        super(RNNLM, self).__init__()\n","        self.embed = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","    \n","    def forward(self, x, h):\n","        # Embed word ids to vectors\n","        x = self.embed(x)\n","\n","        # Forward propagate LSTM\n","        out, (h, c) = self.lstm(x, h)\n","\n","        # Reshape output to (batch_size*sequence_length, hidden_size)\n","        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n","\n","        # Decode hidden states of all time steps\n","        out = self.linear(out)\n","        return out, (h, c)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"rKe19JO3m5Qg","executionInfo":{"status":"ok","timestamp":1618139164949,"user_tz":-540,"elapsed":11488,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}}},"source":["model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"p4snUs8tnAPu","executionInfo":{"status":"ok","timestamp":1618141313408,"user_tz":-540,"elapsed":1372,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}}},"source":["# Truncated backpropagation\n","def detach(states):\n","    return [state.detach() for state in states] # detach() will turn of backprop"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"DD2jBGyavO8p","executionInfo":{"status":"ok","timestamp":1618141416288,"user_tz":-540,"elapsed":1046,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}}},"source":["# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V7l4mXW4voZH","executionInfo":{"status":"ok","timestamp":1618142101497,"user_tz":-540,"elapsed":135769,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}},"outputId":"4623ae5a-2991-43c4-c872-b906c0cc5341"},"source":["# Train the model\n","for epoch in range(num_epochs):\n","    # Set initial hidden and cell states\n","    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n","              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n","    \n","    for i in range(0, ids.size(1) - seq_length, seq_length):\n","        # get mini-batch inputs and targets\n","        inputs = ids[:, i:i+seq_length].to(device)\n","        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n","\n","        # Forward pass\n","        states = detach(states)\n","        outputs, states = model(inputs, states)\n","        loss = criterion(outputs, targets.reshape(-1))\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        clip_grad_norm_(model.parameters(), 0.5)\n","        optimizer.step()\n","\n","        step = (i+1) // seq_length\n","        if step % 100 == 0:\n","            print(\"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}\".format(\n","                epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())\n","            ))\n"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Epoch [1/5], Step [0/1549], Loss: 9.2078, Perplexity: 9974.35\n","Epoch [1/5], Step [100/1549], Loss: 6.0460, Perplexity: 422.41\n","Epoch [1/5], Step [200/1549], Loss: 5.9223, Perplexity: 373.27\n","Epoch [1/5], Step [300/1549], Loss: 5.7275, Perplexity: 307.19\n","Epoch [1/5], Step [400/1549], Loss: 5.6745, Perplexity: 291.35\n","Epoch [1/5], Step [500/1549], Loss: 5.0919, Perplexity: 162.70\n","Epoch [1/5], Step [600/1549], Loss: 5.1938, Perplexity: 180.15\n","Epoch [1/5], Step [700/1549], Loss: 5.3506, Perplexity: 210.73\n","Epoch [1/5], Step [800/1549], Loss: 5.1731, Perplexity: 176.46\n","Epoch [1/5], Step [900/1549], Loss: 5.1073, Perplexity: 165.23\n","Epoch [1/5], Step [1000/1549], Loss: 5.0997, Perplexity: 163.98\n","Epoch [1/5], Step [1100/1549], Loss: 5.3586, Perplexity: 212.43\n","Epoch [1/5], Step [1200/1549], Loss: 5.1714, Perplexity: 176.17\n","Epoch [1/5], Step [1300/1549], Loss: 5.0880, Perplexity: 162.07\n","Epoch [1/5], Step [1400/1549], Loss: 4.8465, Perplexity: 127.29\n","Epoch [1/5], Step [1500/1549], Loss: 5.1632, Perplexity: 174.72\n","Epoch [2/5], Step [0/1549], Loss: 5.4641, Perplexity: 236.05\n","Epoch [2/5], Step [100/1549], Loss: 4.5757, Perplexity: 97.10\n","Epoch [2/5], Step [200/1549], Loss: 4.7122, Perplexity: 111.30\n","Epoch [2/5], Step [300/1549], Loss: 4.6583, Perplexity: 105.46\n","Epoch [2/5], Step [400/1549], Loss: 4.5254, Perplexity: 92.34\n","Epoch [2/5], Step [500/1549], Loss: 4.0846, Perplexity: 59.42\n","Epoch [2/5], Step [600/1549], Loss: 4.4448, Perplexity: 85.18\n","Epoch [2/5], Step [700/1549], Loss: 4.4756, Perplexity: 87.84\n","Epoch [2/5], Step [800/1549], Loss: 4.3749, Perplexity: 79.43\n","Epoch [2/5], Step [900/1549], Loss: 4.2087, Perplexity: 67.27\n","Epoch [2/5], Step [1000/1549], Loss: 4.3242, Perplexity: 75.51\n","Epoch [2/5], Step [1100/1549], Loss: 4.5596, Perplexity: 95.54\n","Epoch [2/5], Step [1200/1549], Loss: 4.4416, Perplexity: 84.91\n","Epoch [2/5], Step [1300/1549], Loss: 4.2440, Perplexity: 69.69\n","Epoch [2/5], Step [1400/1549], Loss: 3.9633, Perplexity: 52.63\n","Epoch [2/5], Step [1500/1549], Loss: 4.3153, Perplexity: 74.83\n","Epoch [3/5], Step [0/1549], Loss: 4.4674, Perplexity: 87.13\n","Epoch [3/5], Step [100/1549], Loss: 3.8790, Perplexity: 48.38\n","Epoch [3/5], Step [200/1549], Loss: 4.0494, Perplexity: 57.36\n","Epoch [3/5], Step [300/1549], Loss: 3.9669, Perplexity: 52.82\n","Epoch [3/5], Step [400/1549], Loss: 3.8820, Perplexity: 48.52\n","Epoch [3/5], Step [500/1549], Loss: 3.3684, Perplexity: 29.03\n","Epoch [3/5], Step [600/1549], Loss: 3.8434, Perplexity: 46.68\n","Epoch [3/5], Step [700/1549], Loss: 3.7536, Perplexity: 42.67\n","Epoch [3/5], Step [800/1549], Loss: 3.7788, Perplexity: 43.76\n","Epoch [3/5], Step [900/1549], Loss: 3.4658, Perplexity: 32.00\n","Epoch [3/5], Step [1000/1549], Loss: 3.6709, Perplexity: 39.29\n","Epoch [3/5], Step [1100/1549], Loss: 3.7953, Perplexity: 44.49\n","Epoch [3/5], Step [1200/1549], Loss: 3.7900, Perplexity: 44.25\n","Epoch [3/5], Step [1300/1549], Loss: 3.5562, Perplexity: 35.03\n","Epoch [3/5], Step [1400/1549], Loss: 3.1880, Perplexity: 24.24\n","Epoch [3/5], Step [1500/1549], Loss: 3.5960, Perplexity: 36.45\n","Epoch [4/5], Step [0/1549], Loss: 3.5724, Perplexity: 35.60\n","Epoch [4/5], Step [100/1549], Loss: 3.3956, Perplexity: 29.83\n","Epoch [4/5], Step [200/1549], Loss: 3.5166, Perplexity: 33.67\n","Epoch [4/5], Step [300/1549], Loss: 3.3975, Perplexity: 29.89\n","Epoch [4/5], Step [400/1549], Loss: 3.3487, Perplexity: 28.47\n","Epoch [4/5], Step [500/1549], Loss: 2.8661, Perplexity: 17.57\n","Epoch [4/5], Step [600/1549], Loss: 3.3492, Perplexity: 28.48\n","Epoch [4/5], Step [700/1549], Loss: 3.2647, Perplexity: 26.17\n","Epoch [4/5], Step [800/1549], Loss: 3.3279, Perplexity: 27.88\n","Epoch [4/5], Step [900/1549], Loss: 3.0013, Perplexity: 20.11\n","Epoch [4/5], Step [1000/1549], Loss: 3.1963, Perplexity: 24.44\n","Epoch [4/5], Step [1100/1549], Loss: 3.2737, Perplexity: 26.41\n","Epoch [4/5], Step [1200/1549], Loss: 3.3007, Perplexity: 27.13\n","Epoch [4/5], Step [1300/1549], Loss: 3.0423, Perplexity: 20.95\n","Epoch [4/5], Step [1400/1549], Loss: 2.7100, Perplexity: 15.03\n","Epoch [4/5], Step [1500/1549], Loss: 3.1352, Perplexity: 22.99\n","Epoch [5/5], Step [0/1549], Loss: 3.0047, Perplexity: 20.18\n","Epoch [5/5], Step [100/1549], Loss: 2.9940, Perplexity: 19.97\n","Epoch [5/5], Step [200/1549], Loss: 3.1965, Perplexity: 24.45\n","Epoch [5/5], Step [300/1549], Loss: 3.0735, Perplexity: 21.62\n","Epoch [5/5], Step [400/1549], Loss: 3.0115, Perplexity: 20.32\n","Epoch [5/5], Step [500/1549], Loss: 2.5338, Perplexity: 12.60\n","Epoch [5/5], Step [600/1549], Loss: 3.0222, Perplexity: 20.54\n","Epoch [5/5], Step [700/1549], Loss: 2.9561, Perplexity: 19.22\n","Epoch [5/5], Step [800/1549], Loss: 2.9600, Perplexity: 19.30\n","Epoch [5/5], Step [900/1549], Loss: 2.7430, Perplexity: 15.53\n","Epoch [5/5], Step [1000/1549], Loss: 2.8277, Perplexity: 16.91\n","Epoch [5/5], Step [1100/1549], Loss: 2.9626, Perplexity: 19.35\n","Epoch [5/5], Step [1200/1549], Loss: 2.9891, Perplexity: 19.87\n","Epoch [5/5], Step [1300/1549], Loss: 2.6993, Perplexity: 14.87\n","Epoch [5/5], Step [1400/1549], Loss: 2.4030, Perplexity: 11.06\n","Epoch [5/5], Step [1500/1549], Loss: 2.8302, Perplexity: 16.95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VwfVtqW_yua5","executionInfo":{"status":"ok","timestamp":1618142248946,"user_tz":-540,"elapsed":1013,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}}},"source":["prob = torch.ones(vocab_size)\n","input = torch.multinomial(prob, num_samples=1).unsqueeze(1)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GJhhgmdyy0sP","executionInfo":{"status":"ok","timestamp":1618142334128,"user_tz":-540,"elapsed":1025,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}},"outputId":"5b3bb33d-4e09-4e09-e127-71f465511973"},"source":["torch.multinomial(prob, num_samples=1)"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([2834])"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Li3-FPFCz70l","executionInfo":{"status":"ok","timestamp":1618142551619,"user_tz":-540,"elapsed":1022,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}},"outputId":"48f670a1-b0f7-4928-9d89-9360e323f8ea"},"source":["input.fill_(0)"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0]])"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DEID-sxxxu05","executionInfo":{"status":"ok","timestamp":1618142782808,"user_tz":-540,"elapsed":1597,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}},"outputId":"c9aef936-71de-42c2-fbfd-07ccfa55c9fd"},"source":["# Test the model\n","with torch.no_grad():\n","    with open('sample.txt', 'w') as f:\n","        # Set initial hidden cell states\n","        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n","                 torch.zeros(num_layers, 1, hidden_size).to(device))\n","        \n","        # Select one word id randomly\n","        prob = torch.ones(vocab_size)\n","        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device) # this returns [[idx]]\n","\n","        for i in range(num_samples):\n","            # Forward propagate RNN\n","            output, state = model(input, state)\n","\n","            # Sample a word id\n","            prob = output.exp()\n","            word_id = torch.multinomial(prob, num_samples=1).item()\n","\n","            # Fill input with sample word id for the next time step\n","            input.fill_(word_id)\n","\n","            # File write\n","            word = corpus.dictionary.idx2word[word_id]\n","            word = '\\n' if word == '<eos>' else word + ' '\n","            f.write(word)\n","\n","            if (i+1) % 100 == 0:\n","                print(\"Sampled [{}/{}] words and save to {}\".format(i+1, num_samples, 'sample.txt'))\n","\n","# Save the model checkpoints\n","torch.save(model.state_dict(), 'model.ckpt')"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Sampled [100/1000] words and save to sample.txt\n","Sampled [200/1000] words and save to sample.txt\n","Sampled [300/1000] words and save to sample.txt\n","Sampled [400/1000] words and save to sample.txt\n","Sampled [500/1000] words and save to sample.txt\n","Sampled [600/1000] words and save to sample.txt\n","Sampled [700/1000] words and save to sample.txt\n","Sampled [800/1000] words and save to sample.txt\n","Sampled [900/1000] words and save to sample.txt\n","Sampled [1000/1000] words and save to sample.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RJ2xKJtz00Dp"},"source":[""],"execution_count":null,"outputs":[]}]}