{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pytorch_Neural_style_transfer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPNfc+LLe/AKV1Mxasyj+bO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"41b4caad58d3401db9f9e3ced67c2525":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cfdba20288fd41288e3bbf23d5a2d656","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_51a1cfe58bcf44ff87ea56a7b97a95dc","IPY_MODEL_603c25e1143c4e53a08308d8c7ca016b"]}},"cfdba20288fd41288e3bbf23d5a2d656":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"51a1cfe58bcf44ff87ea56a7b97a95dc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_82393d762c814cb3ac4e0255e6e66f21","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":574673361,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":574673361,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_587d071dc7074b11bc736238faa5d68b"}},"603c25e1143c4e53a08308d8c7ca016b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_58f76fd8b7414248921c910f2c97122e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 548M/548M [02:54&lt;00:00, 3.30MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_84796b0a661e47b7a84e60faece1f113"}},"82393d762c814cb3ac4e0255e6e66f21":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"587d071dc7074b11bc736238faa5d68b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"58f76fd8b7414248921c910f2c97122e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"84796b0a661e47b7a84e60faece1f113":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"dPyUZ8J9v0_k"},"source":["# Neural Style Transfer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2bnQzPXvxTh","executionInfo":{"status":"ok","timestamp":1618382864329,"user_tz":-540,"elapsed":20464,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}},"outputId":"4239724b-a97d-4a01-edba-ff132dc77805"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uWhlX_Ld3ozZ"},"source":["import os\n","os.chdir(\"./drive/My Drive/Colab Notebooks/pytorch\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fp55E_l8v4sZ"},"source":["from __future__ import division\n","from torchvision import models\n","from torchvision import transforms\n","from PIL import Image\n","import argparse\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uaF_jnw1wLao"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RIssp0n6wf6Z"},"source":["def load_image(image_path, transform=None, max_size=None, shape=None):\n","    # load an image and convert it to a torch tensor\n","    image = Image.open(image_path)\n","\n","    if max_size:\n","        scale = max_size / max(image.size)\n","        size = np.array(image.size) * scale\n","        image = image.resize(size.astype(int), Image.ANTIALIAS) # a high-quality downsampling filter\n","    \n","    if shape:\n","        image = image.resize(shape, Image.LANCZOS) # image upscaling quality \n","    \n","    if transform:\n","        image = transform(image).unsqueeze(0)\n","    \n","    return image.to(device)\n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uDLIjrhoxX0c"},"source":["class VGGNet(nn.Module):\n","    def __init__(self):\n","        # select con1_1 ~ conv5_1 activation maps\n","        super(VGGNet, self).__init__()\n","        self.select = ['0', '5', '10', '19', '28']\n","        self.vgg = models.vgg19(pretrained=True).features\n","    \n","    def forward(self, x):\n","        # extract multiple conv feature maps\n","        features = []\n","        for name, layer in self.vgg._modules.items():\n","            x = layer(x)\n","            if name in self.select:\n","                features.append(x)\n","        return features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":103,"referenced_widgets":["41b4caad58d3401db9f9e3ced67c2525","cfdba20288fd41288e3bbf23d5a2d656","51a1cfe58bcf44ff87ea56a7b97a95dc","603c25e1143c4e53a08308d8c7ca016b","82393d762c814cb3ac4e0255e6e66f21","587d071dc7074b11bc736238faa5d68b","58f76fd8b7414248921c910f2c97122e","84796b0a661e47b7a84e60faece1f113"]},"id":"E5V1AqYiyFsW","executionInfo":{"status":"ok","timestamp":1618382880049,"user_tz":-540,"elapsed":10407,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}},"outputId":"43d3864e-11d5-445a-ace9-5c4905c74919"},"source":["vgg19 = models.vgg19(pretrained=True).features"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"41b4caad58d3401db9f9e3ced67c2525","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=574673361.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KCYNccUYyzDI"},"source":["max_size = 400\n","total_step = 2000\n","log_step = 10\n","sample_step = 500\n","style_weight = 100\n","lr = 0.003\n","\n","content = 'png/eon.jpg'\n","style = 'png/style.png'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xt93q6qTzM4h","executionInfo":{"status":"ok","timestamp":1618388327844,"user_tz":-540,"elapsed":1104391,"user":{"displayName":"Dui Lee","photoUrl":"","userId":"02057638552230258921"}},"outputId":"1b6673b1-b546-4c89-cf22-363561373d75"},"source":["# Image preprocessing\n","# VGGNet was trained on Imagenet where images are normalized by mean=[0.485, 0.456, 0.406]\n","# and std = [0.229, 0.224, 0.225]\n","# We use the same normalization statistics here\n","transform = transforms.Compose([\n","                  transforms.ToTensor(),\n","                  transforms.Normalize(mean=(0.485, 0.456, 0.406),\n","                                       std=(0.229, 0.224, 0.225))\n","])\n","\n","# Load content and style images\n","# make the style image to same size as the content image\n","content = load_image(content, transform, max_size=max_size)\n","style = load_image(style, transform, shape=[content.size(2), content.size(3)])\n","\n","# Initialize a target image with the content image\n","target = content.clone().requires_grad_(True)\n","\n","optimizer = torch.optim.Adam([target], lr=lr, betas=[0.5, 0.999])\n","vgg = VGGNet().to(device).eval()\n","\n","for step in range(total_step):\n","    \n","    # Extract multiple(5) conv feature vectors\n","    target_features = vgg(target)\n","    content_features = vgg(content)\n","    style_features = vgg(style)\n","\n","    style_loss = 0\n","    content_loss = 0\n","\n","    for f1, f2, f3 in zip(target_features, content_features, style_features):\n","        # Compute content loss with target and content images\n","        content_loss += torch.mean((f1-f2)**2)\n","\n","        # Reshape conv feature maps\n","        _, c, h, w = f1.size()\n","        f1 = f1.view(c, h * w)\n","        f3 = f3.view(c, h * w)\n","\n","        # Compute gram matrix\n","        f1 = torch.mm(f1, f1.t())\n","        f3 = torch.mm(f3, f3.t())\n","\n","        # Compute style loss with target and style images\n","        style_loss += torch.mean((f1-f3)**2) / (c*h*w)\n","    \n","    # Compute total loss, backprop and optimize\n","    loss = content_loss + style_weight*style_loss\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (step+1) % log_step == 0:\n","        print('Step [{}/{}], Content Loss: {:.4f}, Style Loss: {:.4f}'.format(\n","            step+1, total_step, content_loss.item(), style_loss.item()\n","        ))\n","    \n","    if (step+1) % sample_step == 0:\n","        # Save the generated image\n","        denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n","        img = target.clone().squeeze()\n","        img = denorm(img).clamp(0, 1)\n","        torchvision.utils.save_image(img, 'png/output_mburg-{}.png'.format(step+1))\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Step [10/2000], Content Loss: 5.5890, Style Loss: 7848.3516\n","Step [20/2000], Content Loss: 16.8720, Style Loss: 6554.2168\n","Step [30/2000], Content Loss: 26.1783, Style Loss: 5345.4106\n","Step [40/2000], Content Loss: 32.0076, Style Loss: 4398.3682\n","Step [50/2000], Content Loss: 35.9323, Style Loss: 3670.0835\n","Step [60/2000], Content Loss: 39.1168, Style Loss: 3106.1875\n","Step [70/2000], Content Loss: 41.8748, Style Loss: 2660.8428\n","Step [80/2000], Content Loss: 44.2729, Style Loss: 2302.4529\n","Step [90/2000], Content Loss: 46.4379, Style Loss: 2009.4473\n","Step [100/2000], Content Loss: 48.4122, Style Loss: 1768.4546\n","Step [110/2000], Content Loss: 50.1882, Style Loss: 1570.0540\n","Step [120/2000], Content Loss: 51.7946, Style Loss: 1405.9554\n","Step [130/2000], Content Loss: 53.2635, Style Loss: 1269.5746\n","Step [140/2000], Content Loss: 54.6113, Style Loss: 1155.6605\n","Step [150/2000], Content Loss: 55.8174, Style Loss: 1059.8309\n","Step [160/2000], Content Loss: 56.9195, Style Loss: 978.4738\n","Step [170/2000], Content Loss: 57.9343, Style Loss: 908.8874\n","Step [180/2000], Content Loss: 58.8633, Style Loss: 848.8791\n","Step [190/2000], Content Loss: 59.7066, Style Loss: 796.5982\n","Step [200/2000], Content Loss: 60.4903, Style Loss: 750.6594\n","Step [210/2000], Content Loss: 61.2207, Style Loss: 709.9791\n","Step [220/2000], Content Loss: 61.8980, Style Loss: 673.6094\n","Step [230/2000], Content Loss: 62.5243, Style Loss: 640.8083\n","Step [240/2000], Content Loss: 63.1109, Style Loss: 611.0895\n","Step [250/2000], Content Loss: 63.6595, Style Loss: 583.9745\n","Step [260/2000], Content Loss: 64.1620, Style Loss: 559.1265\n","Step [270/2000], Content Loss: 64.6306, Style Loss: 536.2495\n","Step [280/2000], Content Loss: 65.0712, Style Loss: 515.1431\n","Step [290/2000], Content Loss: 65.4851, Style Loss: 495.5975\n","Step [300/2000], Content Loss: 65.8761, Style Loss: 477.3986\n","Step [310/2000], Content Loss: 66.2482, Style Loss: 460.4807\n","Step [320/2000], Content Loss: 66.6040, Style Loss: 444.6511\n","Step [330/2000], Content Loss: 66.9509, Style Loss: 429.8118\n","Step [340/2000], Content Loss: 67.2808, Style Loss: 415.8912\n","Step [350/2000], Content Loss: 67.5977, Style Loss: 402.8195\n","Step [360/2000], Content Loss: 67.8938, Style Loss: 390.5176\n","Step [370/2000], Content Loss: 68.1759, Style Loss: 378.9234\n","Step [380/2000], Content Loss: 68.4452, Style Loss: 367.9771\n","Step [390/2000], Content Loss: 68.7017, Style Loss: 357.6396\n","Step [400/2000], Content Loss: 68.9470, Style Loss: 347.8434\n","Step [410/2000], Content Loss: 69.1838, Style Loss: 338.5215\n","Step [420/2000], Content Loss: 69.4119, Style Loss: 329.6671\n","Step [430/2000], Content Loss: 69.6287, Style Loss: 321.2552\n","Step [440/2000], Content Loss: 69.8391, Style Loss: 313.2323\n","Step [450/2000], Content Loss: 70.0424, Style Loss: 305.5857\n","Step [460/2000], Content Loss: 70.2390, Style Loss: 298.2789\n","Step [470/2000], Content Loss: 70.4264, Style Loss: 291.2980\n","Step [480/2000], Content Loss: 70.6057, Style Loss: 284.6396\n","Step [490/2000], Content Loss: 70.7818, Style Loss: 278.2770\n","Step [500/2000], Content Loss: 70.9536, Style Loss: 272.1841\n","Step [510/2000], Content Loss: 71.1197, Style Loss: 266.3559\n","Step [520/2000], Content Loss: 71.2787, Style Loss: 260.7625\n","Step [530/2000], Content Loss: 71.4282, Style Loss: 255.3885\n","Step [540/2000], Content Loss: 71.5760, Style Loss: 250.2318\n","Step [550/2000], Content Loss: 71.7222, Style Loss: 245.2856\n","Step [560/2000], Content Loss: 71.8599, Style Loss: 240.5231\n","Step [570/2000], Content Loss: 71.9928, Style Loss: 235.9306\n","Step [580/2000], Content Loss: 72.1245, Style Loss: 231.5024\n","Step [590/2000], Content Loss: 72.2554, Style Loss: 227.2214\n","Step [600/2000], Content Loss: 72.3823, Style Loss: 223.0860\n","Step [610/2000], Content Loss: 72.5089, Style Loss: 219.0938\n","Step [620/2000], Content Loss: 72.6341, Style Loss: 215.2373\n","Step [630/2000], Content Loss: 72.7550, Style Loss: 211.5054\n","Step [640/2000], Content Loss: 72.8725, Style Loss: 207.8888\n","Step [650/2000], Content Loss: 72.9896, Style Loss: 204.3840\n","Step [660/2000], Content Loss: 73.1052, Style Loss: 200.9862\n","Step [670/2000], Content Loss: 73.2153, Style Loss: 197.6837\n","Step [680/2000], Content Loss: 73.3251, Style Loss: 194.4834\n","Step [690/2000], Content Loss: 73.4340, Style Loss: 191.3804\n","Step [700/2000], Content Loss: 73.5378, Style Loss: 188.3647\n","Step [710/2000], Content Loss: 73.6405, Style Loss: 185.4305\n","Step [720/2000], Content Loss: 73.7410, Style Loss: 182.5760\n","Step [730/2000], Content Loss: 73.8391, Style Loss: 179.7931\n","Step [740/2000], Content Loss: 73.9375, Style Loss: 177.0814\n","Step [750/2000], Content Loss: 74.0352, Style Loss: 174.4408\n","Step [760/2000], Content Loss: 74.1316, Style Loss: 171.8668\n","Step [770/2000], Content Loss: 74.2249, Style Loss: 169.3592\n","Step [780/2000], Content Loss: 74.3202, Style Loss: 166.9135\n","Step [790/2000], Content Loss: 74.4134, Style Loss: 164.5302\n","Step [800/2000], Content Loss: 74.5070, Style Loss: 162.1994\n","Step [810/2000], Content Loss: 74.5981, Style Loss: 159.9277\n","Step [820/2000], Content Loss: 74.6873, Style Loss: 157.7105\n","Step [830/2000], Content Loss: 74.7746, Style Loss: 155.5477\n","Step [840/2000], Content Loss: 74.8588, Style Loss: 153.4329\n","Step [850/2000], Content Loss: 74.9401, Style Loss: 151.3665\n","Step [860/2000], Content Loss: 75.0222, Style Loss: 149.3542\n","Step [870/2000], Content Loss: 75.1020, Style Loss: 147.3902\n","Step [880/2000], Content Loss: 75.1792, Style Loss: 145.4685\n","Step [890/2000], Content Loss: 75.2545, Style Loss: 143.5867\n","Step [900/2000], Content Loss: 75.3293, Style Loss: 141.7474\n","Step [910/2000], Content Loss: 75.4017, Style Loss: 139.9476\n","Step [920/2000], Content Loss: 75.4732, Style Loss: 138.1873\n","Step [930/2000], Content Loss: 75.5447, Style Loss: 136.4656\n","Step [940/2000], Content Loss: 75.6135, Style Loss: 134.7807\n","Step [950/2000], Content Loss: 75.6796, Style Loss: 133.1304\n","Step [960/2000], Content Loss: 75.7459, Style Loss: 131.5152\n","Step [970/2000], Content Loss: 75.8102, Style Loss: 129.9307\n","Step [980/2000], Content Loss: 75.8774, Style Loss: 128.3766\n","Step [990/2000], Content Loss: 75.9437, Style Loss: 126.8488\n","Step [1000/2000], Content Loss: 76.0089, Style Loss: 125.3489\n","Step [1010/2000], Content Loss: 76.0731, Style Loss: 123.8771\n","Step [1020/2000], Content Loss: 76.1363, Style Loss: 122.4366\n","Step [1030/2000], Content Loss: 76.1960, Style Loss: 121.0257\n","Step [1040/2000], Content Loss: 76.2561, Style Loss: 119.6427\n","Step [1050/2000], Content Loss: 76.3148, Style Loss: 118.2889\n","Step [1060/2000], Content Loss: 76.3732, Style Loss: 116.9613\n","Step [1070/2000], Content Loss: 76.4316, Style Loss: 115.6607\n","Step [1080/2000], Content Loss: 76.4884, Style Loss: 114.3830\n","Step [1090/2000], Content Loss: 76.5435, Style Loss: 113.1290\n","Step [1100/2000], Content Loss: 76.5997, Style Loss: 111.8974\n","Step [1110/2000], Content Loss: 76.6572, Style Loss: 110.6879\n","Step [1120/2000], Content Loss: 76.7142, Style Loss: 109.5003\n","Step [1130/2000], Content Loss: 76.7689, Style Loss: 108.3324\n","Step [1140/2000], Content Loss: 76.8230, Style Loss: 107.1852\n","Step [1150/2000], Content Loss: 76.8767, Style Loss: 106.0586\n","Step [1160/2000], Content Loss: 76.9297, Style Loss: 104.9529\n","Step [1170/2000], Content Loss: 76.9819, Style Loss: 103.8663\n","Step [1180/2000], Content Loss: 77.0328, Style Loss: 102.7991\n","Step [1190/2000], Content Loss: 77.0842, Style Loss: 101.7496\n","Step [1200/2000], Content Loss: 77.1341, Style Loss: 100.7171\n","Step [1210/2000], Content Loss: 77.1840, Style Loss: 99.7003\n","Step [1220/2000], Content Loss: 77.2345, Style Loss: 98.7007\n","Step [1230/2000], Content Loss: 77.2831, Style Loss: 97.7177\n","Step [1240/2000], Content Loss: 77.3317, Style Loss: 96.7510\n","Step [1250/2000], Content Loss: 77.3812, Style Loss: 95.7989\n","Step [1260/2000], Content Loss: 77.4290, Style Loss: 94.8604\n","Step [1270/2000], Content Loss: 77.4764, Style Loss: 93.9372\n","Step [1280/2000], Content Loss: 77.5235, Style Loss: 93.0287\n","Step [1290/2000], Content Loss: 77.5697, Style Loss: 92.1353\n","Step [1300/2000], Content Loss: 77.6152, Style Loss: 91.2561\n","Step [1310/2000], Content Loss: 77.6615, Style Loss: 90.3924\n","Step [1320/2000], Content Loss: 77.7073, Style Loss: 89.5443\n","Step [1330/2000], Content Loss: 77.7509, Style Loss: 88.7093\n","Step [1340/2000], Content Loss: 77.7933, Style Loss: 87.8884\n","Step [1350/2000], Content Loss: 77.8354, Style Loss: 87.0812\n","Step [1360/2000], Content Loss: 77.8774, Style Loss: 86.2872\n","Step [1370/2000], Content Loss: 77.9177, Style Loss: 85.5053\n","Step [1380/2000], Content Loss: 77.9573, Style Loss: 84.7352\n","Step [1390/2000], Content Loss: 77.9974, Style Loss: 83.9760\n","Step [1400/2000], Content Loss: 78.0373, Style Loss: 83.2276\n","Step [1410/2000], Content Loss: 78.0769, Style Loss: 82.4905\n","Step [1420/2000], Content Loss: 78.1160, Style Loss: 81.7647\n","Step [1430/2000], Content Loss: 78.1549, Style Loss: 81.0490\n","Step [1440/2000], Content Loss: 78.1945, Style Loss: 80.3433\n","Step [1450/2000], Content Loss: 78.2338, Style Loss: 79.6476\n","Step [1460/2000], Content Loss: 78.2721, Style Loss: 78.9619\n","Step [1470/2000], Content Loss: 78.3113, Style Loss: 78.2865\n","Step [1480/2000], Content Loss: 78.3498, Style Loss: 77.6223\n","Step [1490/2000], Content Loss: 78.3882, Style Loss: 76.9676\n","Step [1500/2000], Content Loss: 78.4249, Style Loss: 76.3205\n","Step [1510/2000], Content Loss: 78.4621, Style Loss: 75.6818\n","Step [1520/2000], Content Loss: 78.5001, Style Loss: 75.0528\n","Step [1530/2000], Content Loss: 78.5369, Style Loss: 74.4333\n","Step [1540/2000], Content Loss: 78.5739, Style Loss: 73.8230\n","Step [1550/2000], Content Loss: 78.6103, Style Loss: 73.2215\n","Step [1560/2000], Content Loss: 78.6453, Style Loss: 72.6289\n","Step [1570/2000], Content Loss: 78.6808, Style Loss: 72.0454\n","Step [1580/2000], Content Loss: 78.7154, Style Loss: 71.4696\n","Step [1590/2000], Content Loss: 78.7491, Style Loss: 70.9023\n","Step [1600/2000], Content Loss: 78.7828, Style Loss: 70.3432\n","Step [1610/2000], Content Loss: 78.8158, Style Loss: 69.7917\n","Step [1620/2000], Content Loss: 78.8487, Style Loss: 69.2463\n","Step [1630/2000], Content Loss: 78.8818, Style Loss: 68.7088\n","Step [1640/2000], Content Loss: 78.9133, Style Loss: 68.1783\n","Step [1650/2000], Content Loss: 78.9459, Style Loss: 67.6553\n","Step [1660/2000], Content Loss: 78.9780, Style Loss: 67.1399\n","Step [1670/2000], Content Loss: 79.0110, Style Loss: 66.6317\n","Step [1680/2000], Content Loss: 79.0423, Style Loss: 66.1303\n","Step [1690/2000], Content Loss: 79.0747, Style Loss: 65.6346\n","Step [1700/2000], Content Loss: 79.1052, Style Loss: 65.1450\n","Step [1710/2000], Content Loss: 79.1368, Style Loss: 64.6622\n","Step [1720/2000], Content Loss: 79.1676, Style Loss: 64.1853\n","Step [1730/2000], Content Loss: 79.1991, Style Loss: 63.7138\n","Step [1740/2000], Content Loss: 79.2321, Style Loss: 63.2483\n","Step [1750/2000], Content Loss: 79.2639, Style Loss: 62.7892\n","Step [1760/2000], Content Loss: 79.2949, Style Loss: 62.3360\n","Step [1770/2000], Content Loss: 79.3257, Style Loss: 61.8894\n","Step [1780/2000], Content Loss: 79.3572, Style Loss: 61.4490\n","Step [1790/2000], Content Loss: 79.3883, Style Loss: 61.0150\n","Step [1800/2000], Content Loss: 79.4191, Style Loss: 60.5872\n","Step [1810/2000], Content Loss: 79.4507, Style Loss: 60.1648\n","Step [1820/2000], Content Loss: 79.4813, Style Loss: 59.7474\n","Step [1830/2000], Content Loss: 79.5119, Style Loss: 59.3354\n","Step [1840/2000], Content Loss: 79.5394, Style Loss: 58.9288\n","Step [1850/2000], Content Loss: 79.5678, Style Loss: 58.5277\n","Step [1860/2000], Content Loss: 79.5946, Style Loss: 58.1316\n","Step [1870/2000], Content Loss: 79.6209, Style Loss: 57.7413\n","Step [1880/2000], Content Loss: 79.6463, Style Loss: 57.3556\n","Step [1890/2000], Content Loss: 79.6724, Style Loss: 56.9736\n","Step [1900/2000], Content Loss: 79.6986, Style Loss: 56.5964\n","Step [1910/2000], Content Loss: 79.7249, Style Loss: 56.2238\n","Step [1920/2000], Content Loss: 79.7511, Style Loss: 55.8549\n","Step [1930/2000], Content Loss: 79.7764, Style Loss: 55.4901\n","Step [1940/2000], Content Loss: 79.8010, Style Loss: 55.1299\n","Step [1950/2000], Content Loss: 79.8266, Style Loss: 54.7744\n","Step [1960/2000], Content Loss: 79.8508, Style Loss: 54.4231\n","Step [1970/2000], Content Loss: 79.8747, Style Loss: 54.0756\n","Step [1980/2000], Content Loss: 79.8998, Style Loss: 53.7316\n","Step [1990/2000], Content Loss: 79.9237, Style Loss: 53.3918\n","Step [2000/2000], Content Loss: 79.9480, Style Loss: 53.0557\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xetEx8P23l-c"},"source":[""],"execution_count":null,"outputs":[]}]}